#!/bin/bash -e

cwd=$(pwd)
rootdir=$HOME/datamount/surge
workdir=$rootdir/tmp
archive=$rootdir/archive
datadir=$rootdir/data
mkdir -p $workdir $archive $datadir

# List of NOAA station ids to pull.  For example of UI for this data:
# See https://slosh.nws.noaa.gov/etsurge2.0/index.php?stid=8419317&datum=MLLW&show=0-0-1-1-0
# Edit this whenever a reserve is added/subtracted.
stations=("8419317" "8658163")

# Top level directories are named "etss.<date>", e.t. etss.20251029, and the most recent 4
# are kept. Each contains several varieties of tar files, with the time "cycle" (0, 6, 12 or 18) 
# embedded in the name. The one we want is etss.t${cycle}z.csv.tar.gz.  Calculate that file 
# name based on the current date/time.
filedate=$(date +%Y%m%d)
hour=$(date +%H)
if [ $hour -ge 23 ]; then
    cycle='18'
elif [ $hour -ge 17 ]; then
    cycle='12'
elif [ $hour -ge 11 ]; then
    cycle='06'
elif [ $hour -ge 5 ]; then
    cycle='00'
else
    # Use yesterday's last cycle
    filedate=$(date -d '-1 day' +%Y%m%d)
    cycle='18'
fi

# This is the expected most recent tar file.
rawfile=etss.t${cycle}z.csv.tar.gz

cd $workdir
rm -rf *  # clean up files from previous run 
url="https://nomads.ncep.noaa.gov/pub/data/nccf/com/petss/prod/etss.${filedate}/${rawfile}"
curl --no-progress-meter --fail-with-body -o $rawfile $url || {
  echo "Failed to download $url"
  exit 1
}
tar zxf $rawfile || {
  echo "Failed to extract $rawfile"
  exit 1
}

# At this point we should have:
# etss.<date>/
#    t<cycle>z.csv/
#       <station>.csv ... for all stations

for ((i=0; i<${#stations[@]}; i++)); do
  datafile=etss.$filedate/t${cycle}z.csv/${stations[$i]}.csv
  if [ ! -f $datafile ]; then
    echo "Expected file $datafile not found after extraction."
  else
    cp $datafile $datadir
    echo "extracted ${stations[$i]}.csv..."
  fi
done

# We'll leave the tmp dir contents around for debugging.
# rm -rf $workdir/*

echo "$(date): $filedate $cycle" >> $rootdir/log

exit 0
